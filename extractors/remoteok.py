from bs4 import BeautifulSoup
import csv
import logging
from playwright.sync_api import sync_playwright
import time
from extractors.job_data_remoteok import JobDataRemoteOK

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RemoteOKJobSearch:
    """
    RemoteOK ì›¹ì‚¬ì´íŠ¸ì—ì„œ êµ¬ì§ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.keywords = []
        self.base_url = "https://remoteok.com/remote-{}-jobs"
        self.proxies = [
            # ì˜ˆì‹œ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸ (ì‹¤ì œ ì‚¬ìš©í•  ë•ŒëŠ” ìœ íš¨í•œ í”„ë¡ì‹œë¡œ êµì²´í•´ì•¼ í•¨)
            # ë¬´ë£Œ í”„ë¡ì‹œëŠ” ìì£¼ ë³€ê²½ë˜ë¯€ë¡œ ì‚¬ìš© ì „ ì²´í¬ í•„ìš”
            "103.149.162.195:80", 
            "47.242.3.214:8081",
            "135.181.29.13:3128"
        ]
        self.current_proxy_index = 0
        
    def add_keyword(self, keyword):
        """ë‹¨ì¼ í‚¤ì›Œë“œ ì¶”ê°€"""
        self.keywords.append(keyword.strip())
        
    def add_keywords_from_input(self, keyword_input):
        """ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œ ì¶”ê°€"""
        keywords = keyword_input.split(',')
        for keyword in keywords:
            self.add_keyword(keyword)
            
    def get_proxy(self):
        """í”„ë¡ì‹œ ì„œë²„ ëª©ë¡ì—ì„œ ë‹¤ìŒ í”„ë¡ì‹œ ë°˜í™˜"""
        if not self.proxies:
            return None
            
        proxy = self.proxies[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxies)
        return proxy
        
    def run_playwright(self, url, scroll_count=4, scroll_delay=5, retry_count=2, use_proxy=False, manual_captcha=False):
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í˜ì´ì§€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        logger.info(f"RemoteOK ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘: {url}")
        
        for attempt in range(retry_count + 1):
            try:
                with sync_playwright() as p:
                    # í”„ë¡ì‹œ ì„¤ì •
                    proxy_settings = None
                    if use_proxy:
                        proxy = self.get_proxy()
                        if proxy:
                            logger.info(f"í”„ë¡ì‹œ ì„œë²„ ì‚¬ìš©: {proxy}")
                            proxy_settings = {
                                "server": f"http://{proxy}"
                                # í•„ìš”ì‹œ ì¸ì¦ ì¶”ê°€:
                                # "username": "ì‚¬ìš©ìëª…",
                                # "password": "ë¹„ë°€ë²ˆí˜¸"
                            }
                    
                    # 1. ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì„¤ì •
                    browser = p.chromium.launch(
                        headless=True,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™” (ë´‡ ê°ì§€ íšŒí”¼ì— ë„ì›€)
                        args=[
                            '--disable-blink-features=AutomationControlled',  # ìë™í™” ê°ì§€ ë¹„í™œì„±í™”
                            '--no-sandbox',
                            '--disable-setuid-sandbox',
                            '--disable-infobars',
                            '--disable-dev-shm-usage',
                            '--disable-accelerated-2d-canvas',
                            '--no-first-run',
                            '--no-zygote',
                            '--disable-gpu'
                        ]
                    )
                    
                    # 2. ë” í˜„ì‹¤ì ì¸ ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
                    context = browser.new_context(
                        viewport={'width': 1920, 'height': 1080},  # ì¼ë°˜ì ì¸ ë°ìŠ¤í¬í†± í•´ìƒë„
                        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
                        locale='ko-KR',  # í•œêµ­ì–´ ë¡œì¼€ì¼ ì„¤ì •
                        proxy=proxy_settings
                    )
                    
                    # ë” ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
                    context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => false,
                    });
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5],
                    });
                    """)
                    
                    page = context.new_page()
                    
                    # 3. ë” ì¸ê°„ì ì¸ í—¤ë” ì„¤ì •
                    page.set_extra_http_headers({
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
                        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                        'Referer': 'https://www.google.com/',  # êµ¬ê¸€ì—ì„œ ë°©ë¬¸í•œ ê²ƒì²˜ëŸ¼ ì„¤ì •
                        'sec-ch-ua': '"Not.A/Brand";v="8", "Chromium";v="111", "Google Chrome";v="111"',
                        'sec-ch-ua-platform': '"Windows"',
                        'sec-ch-ua-mobile': '?0'
                    })
                    
                    # 4. ì¿ í‚¤ í—ˆìš© ë° ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì„¤ì •
                    page.goto(url)
                    
                    # 5. í•„ìš”ì‹œ ì¿ í‚¤ ìˆ˜ë½ ë²„íŠ¼ í´ë¦­ (ì‚¬ì´íŠ¸ì— ì¿ í‚¤ ë‹¤ì´ì–¼ë¡œê·¸ê°€ ìˆëŠ” ê²½ìš°)
                    try:
                        cookie_buttons = page.locator('button:has-text("Accept") , button:has-text("Agree"), button:has-text("Accept all cookies")')
                        if cookie_buttons.count() > 0:
                            cookie_buttons.first.click()
                            logger.info("ì¿ í‚¤ ìˆ˜ë½ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
                    except Exception as e:
                        logger.info(f"ì¿ í‚¤ ìˆ˜ë½ ë²„íŠ¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                    
                    # 6. ì¸ê°„ì²˜ëŸ¼ ë™ì‘í•˜ê¸° ìœ„í•œ ë¬´ì‘ìœ„ ëŒ€ê¸°
                    time.sleep(2 + 2 * (0.1 * time.time() % 1.0))  # 2-4ì´ˆ ëœë¤ ëŒ€ê¸°
                    
                    # 7. ë¡œë´‡ ì²´í¬/ìº¡ì°¨ ê°ì§€ ë° ì²˜ë¦¬
                    page_content = page.content().lower()
                    if "captcha" in page_content or "robot" in page_content or "cloudflare" in page_content:
                        logger.warning(f"ìº¡ì°¨ ë˜ëŠ” ë¡œë´‡ ì²´í¬ ê°ì§€! ì‹œë„ {attempt+1}/{retry_count+1}")
                        
                        # ìˆ˜ë™ ìº¡ì°¨ í•´ê²° ì˜µì…˜ì´ ì¼œì ¸ ìˆëŠ” ê²½ìš°
                        if manual_captcha:
                            try:
                                # ìº¡ì°¨ í™”ë©´ ì €ì¥
                                screenshot_path = f"captcha_detected_{attempt}.png"
                                page.screenshot(path=screenshot_path)
                                logger.info(f"ìº¡ì°¨ í™”ë©´ì´ {screenshot_path}ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                                
                                # ì‚¬ìš©ìì—ê²Œ ìº¡ì°¨ í•´ê²° ìš”ì²­
                                import os
                                print("\n" + "="*60)
                                print(f"ìº¡ì°¨ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! ìŠ¤í¬ë¦°ìƒ·: {os.path.abspath(screenshot_path)}")
                                print("ë¸Œë¼ìš°ì € ì°½ì—ì„œ ìº¡ì°¨ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”.")
                                print("í•´ê²° í›„ ì•„ë¬´ í‚¤ë‚˜ ì…ë ¥í•˜ë©´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                                print("="*60 + "\n")
                                
                                # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
                                input("ìº¡ì°¨ í•´ê²° í›„ Enter í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                                
                                # ìº¡ì°¨ê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                updated_content = page.content().lower()
                                if "captcha" in updated_content or "robot" in updated_content:
                                    logger.warning("ìº¡ì°¨ê°€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                                    # ì„¸ì…˜ ì¢…ë£Œ í›„ ì¬ì‹œë„
                                    context.close()
                                    browser.close()
                                    time.sleep(2)
                                    continue
                                else:
                                    logger.info("ìº¡ì°¨ê°€ ì„±ê³µì ìœ¼ë¡œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            except Exception as e:
                                logger.error(f"ìº¡ì°¨ ìˆ˜ë™ í•´ê²° ì¤‘ ì˜¤ë¥˜: {e}")
                                # ìˆ˜ë™ í•´ê²° ì‹¤íŒ¨ ì‹œ ìë™ ì²˜ë¦¬ ì‹œë„
                        else:
                            # ìë™ ì²˜ë¦¬: ìº¡ì°¨ í™”ë©´ ì €ì¥ í›„ ì„¸ì…˜ ì¢…ë£Œ
                            try:
                                # ìº¡ì°¨ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹… ìš©ë„)
                                page.screenshot(path=f"captcha_detected_{attempt}.png")
                                logger.info(f"ìº¡ì°¨ í™”ë©´ì´ captcha_detected_{attempt}.pngë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                                
                                # ì´ ì‹œë„ ì‹¤íŒ¨ ì²˜ë¦¬
                                context.close()
                                browser.close()
                                
                                if attempt < retry_count:
                                    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ ë³€ê²½
                                    use_proxy = not use_proxy
                                    logger.info(f"ë‹¤ìŒ ì‹œë„ì—ì„œ í”„ë¡ì‹œ ì‚¬ìš©: {use_proxy}")
                                    time.sleep(5 + 5 * attempt)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                                    continue
                                else:
                                    logger.error("ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ - ìº¡ì°¨ ë˜ëŠ” ë´‡ ê°ì§€ë¡œ ì¸í•´ ì°¨ë‹¨ë¨")
                                    return None
                            except Exception as e:
                                logger.error(f"ìº¡ì°¨ í™”ë©´ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ ë” ë§ì€ êµ¬ì§ ì •ë³´ ë¡œë“œ - ì‚¬ëŒì²˜ëŸ¼ ë¶€ë“œëŸ½ê²Œ ìŠ¤í¬ë¡¤
                    for i in range(scroll_count):
                        logger.info(f"ìŠ¤í¬ë¡¤ ë‹¤ìš´ {i+1}/{scroll_count}")
                        
                        # ë¶€ë“œëŸ¬ìš´ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ (ì¼ë°˜ End í‚¤ ëŒ€ì‹ )
                        current_height = page.evaluate("document.body.scrollHeight")
                        target_height = current_height * (i + 1) / scroll_count
                        
                        # ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“¤ê¸°
                        steps = 10
                        for step in range(1, steps + 1):
                            page.evaluate(f"window.scrollTo(0, {target_height * step / steps})")
                            # ë¶ˆê·œì¹™í•œ ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ëŒ€ê¸°
                            time.sleep(0.1 + 0.2 * (0.1 * time.time() % 1.0))
                        
                        # ìŠ¤í¬ë¡¤ í›„ ë¬´ì‘ìœ„ ì‹œê°„ ëŒ€ê¸°
                        wait_time = scroll_delay * (0.8 + 0.4 * (0.1 * time.time() % 1.0))
                        time.sleep(wait_time)
                    
                    # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ - ëª¨ë“  ë™ì  ì½˜í…ì¸ ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
                    logger.info("ì¶”ê°€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
                    time.sleep(3)
                    
                    # ë””ë²„ê¹…: HTML êµ¬ì¡° í™•ì¸
                    html_content = page.content()
                    soup_debug = BeautifulSoup(html_content, 'html.parser')
                    
                    # í…Œì´ë¸” ì°¾ê¸° ì‹œë„
                    tables = soup_debug.find_all('table')
                    logger.info(f"í˜ì´ì§€ì—ì„œ {len(tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
                    
                    for i, table in enumerate(tables):
                        table_id = table.get('id', 'ì—†ìŒ')
                        logger.info(f"í…Œì´ë¸” {i+1} ID: {table_id}")
                    
                    # jobsboard í…Œì´ë¸” ì°¾ê¸° ì‹œë„
                    jobsboard_debug = soup_debug.find('table', id='jobsboard')
                    if jobsboard_debug:
                        logger.info("jobsboard í…Œì´ë¸”ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                        job_rows = jobsboard_debug.find_all('tr', class_='job')
                        logger.info(f"job í´ë˜ìŠ¤ í–‰ {len(job_rows)}ê°œ ë°œê²¬")
                        
                        # ì„±ê³µ! ì´ ì‹œë„ ì™„ë£Œ
                        content = page.content()
                        context.close()
                        browser.close()
                        logger.info("í˜ì´ì§€ ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ")
                        return content
                    else:
                        logger.warning("ë””ë²„ê¹…: jobsboard í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        
                        # ì‹¤íŒ¨í•œ ê²½ìš°, ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
                        if attempt < retry_count:
                            context.close()
                            browser.close()
                            logger.info(f"jobsboardë¥¼ ì°¾ì§€ ëª»í•¨, ì¬ì‹œë„ ì¤‘ ({attempt+1}/{retry_count+1})")
                            # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ ë³€ê²½
                            use_proxy = not use_proxy
                            time.sleep(5 + 5 * attempt)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                            continue
                    
                    # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” ê²°ê³¼ ë°˜í™˜
                    content = page.content()
                    context.close()
                    browser.close()
                    logger.info("í˜ì´ì§€ ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ (í…Œì´ë¸” ë¯¸ë°œê²¬)")
                    return content
                    
            except Exception as e:
                logger.error(f"Playwright ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt+1}/{retry_count+1}): {e}")
                if attempt < retry_count:
                    logger.info(f"5ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(5)
                    continue
                else:
                    return None
        
        return None
            
    def scrape_keyword(self, keyword, manual_captcha=False):
        """íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ êµ¬ì§ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        jobs_db = []
        url = self.base_url.format(keyword)
        
        logger.info(f"RemoteOK ìŠ¤í¬ë˜í•‘ ì‹œì‘: URL = {url}")
        
        # ë¨¼ì € í”„ë¡ì‹œ ì—†ì´ ì‹œë„
        content = self.run_playwright(url, use_proxy=False, manual_captcha=manual_captcha)
        
        # ì‹¤íŒ¨í•˜ë©´ í”„ë¡ì‹œë¡œ ì‹œë„
        if not content:
            logger.warning("í”„ë¡ì‹œ ì—†ì´ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨, í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            content = self.run_playwright(url, use_proxy=True, manual_captcha=manual_captcha)
            
        if not content:
            logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return jobs_db
            
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # ë³€ê²½ëœ ë°©ì‹: ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì±„ìš©ê³µê³  í…Œì´ë¸” ì°¾ê¸° ì‹œë„
            jobsboard = soup.find('table', id='jobsboard')
            
            # ë°©ë²• 1 ì‹¤íŒ¨ ì‹œ, ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            if not jobsboard:
                logger.warning("jobsboard IDë¡œ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
                # ëª¨ë“  í…Œì´ë¸” í™•ì¸
                tables = soup.find_all('table')
                logger.info(f"{len(tables)}ê°œì˜ í…Œì´ë¸”ì´ ë°œê²¬ë¨")
                
                # ê°€ì¥ ë§ì€ í–‰ì„ ê°€ì§„ í…Œì´ë¸”ì„ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ êµ¬ì§ ì •ë³´ í…Œì´ë¸”)
                if tables:
                    max_rows = 0
                    for table in tables:
                        rows = table.find_all('tr')
                        if len(rows) > max_rows:
                            max_rows = len(rows)
                            jobsboard = table
                    
                    if jobsboard:
                        logger.info(f"ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ í…Œì´ë¸”ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {max_rows}ê°œì˜ í–‰ì´ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        logger.warning("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œë„ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            if not jobsboard:
                # ë§ˆì§€ë§‰ ë°©ë²•: ì§ì ‘ tr ìš”ì†Œ ì°¾ê¸°
                logger.warning("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì§ì ‘ tr.job ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
                job_rows = soup.find_all('tr', class_='job')
                
                if job_rows:
                    logger.info(f"í…Œì´ë¸” ì—†ì´ {len(job_rows)}ê°œì˜ job í–‰ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                else:
                    # ì™„ì „íˆ ë‹¤ë¥¸ ì ‘ê·¼ë²•: classê°€ ì—†ëŠ” tr ìš”ì†Œë“¤ë„ ê²€ìƒ‰
                    job_rows = soup.find_all('tr')
                    # data-id ë˜ëŠ” data-slug ì†ì„±ì´ ìˆëŠ” í–‰ë§Œ í•„í„°ë§
                    job_rows = [row for row in job_rows if row.get('data-id') or row.get('data-slug')]
                    logger.info(f"ì†ì„± ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ {len(job_rows)}ê°œì˜ tr í–‰ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                    
                    if not job_rows:
                        logger.warning("êµ¬ì§ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return jobs_db
            else:
                # <tbody> ì•„ë˜ì˜ ëª¨ë“  tr ìš”ì†Œ ì°¾ê¸° (ê° ìš”ì†ŒëŠ” í•˜ë‚˜ì˜ ì±„ìš©ê³µê³ )
                job_rows = jobsboard.find_all('tr')
                # í•„í„°ë§: ê´‘ê³ , êµ¬ë¶„ì„ , ì¼ë°˜ ì‘ì—… í–‰ë§Œ ë‚¨ê¸°ê¸°
                job_rows = [row for row in job_rows if 
                          row.get('class') and ('job' in row.get('class') or 
                                              'sw-insert' in row.get('class')) or
                          row.get('data-id') or row.get('data-slug')]
                
                if not job_rows:
                    logger.warning("êµ¬ì§ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return jobs_db
            
            logger.info(f"ì²˜ë¦¬í•  ì±„ìš©ê³µê³  ìˆ˜: {len(job_rows)}")
            
            for job_row in job_rows:
                try:
                    # í–‰ì´ ê´‘ê³ (sw-insert)ì¸ì§€ í™•ì¸
                    is_ad = False
                    if job_row.get('class') and 'sw-insert' in job_row.get('class'):
                        is_ad = True
                        logger.info("ê´‘ê³  í•­ëª© ë°œê²¬")
                    
                    # ê° ê³µê³ ì˜ data-slug ì†ì„±ì—ì„œ URL ìŠ¬ëŸ¬ê·¸ ì¶”ì¶œ
                    job_slug = job_row.get('data-slug', '')
                    job_id = job_row.get('data-id', '')
                    
                    # URL ì„¤ì •: ê´‘ê³ ì™€ ì¼ë°˜ ê³µê³ ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                    if is_ad:
                        # ê´‘ê³ ì˜ ê²½ìš° a íƒœê·¸ì—ì„œ URL ì¶”ì¶œ
                        link_element = job_row.find('a')
                        url = link_element.get('href', '') if link_element else ""
                        # URLì´ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì „ì²´ URLë¡œ ë³€í™˜
                        if url and not url.startswith('http'):
                            url = f"https://remoteok.com{url}"
                    else:
                        # ì¼ë°˜ ê³µê³ ëŠ” data-slug ì‚¬ìš©
                        url = f"https://remoteok.com/remote-jobs/{job_slug}" if job_slug else ""
                        # data-slugê°€ ì—†ëŠ” ê²½ìš° data-id ì‚¬ìš©
                        if not url and job_id:
                            url = f"https://remoteok.com/remote-jobs/{job_id}"
                    
                    # URLì´ ì—†ìœ¼ë©´ ê³„ì† ì§„í–‰
                    if not url:
                        logger.warning("URLì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # company_and_position í´ë˜ìŠ¤ë¥¼ ê°€ì§„ td ìš”ì†Œ ì°¾ê¸°
                    company_position_cell = job_row.find('td', class_='company_and_position')
                    
                    # ëŒ€ì²´ ë°©ë²•: company position í´ë˜ìŠ¤ë¡œ ì°¾ê¸°
                    if not company_position_cell:
                        company_position_cell = job_row.find('td', class_='company position company_and_position')
                    
                    if not company_position_cell:
                        logger.warning("íšŒì‚¬ ë° ì§ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # ì œëª©(title) ì¶”ì¶œ
                    if is_ad:
                        # ê´‘ê³ ì˜ ê²½ìš° strong íƒœê·¸ ë˜ëŠ” a íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        title_element = company_position_cell.find('strong')
                        if not title_element:
                            title_element = company_position_cell.find('a')
                        title = title_element.text.strip() if title_element else "ê´‘ê³ "
                    else:
                        # ì¼ë°˜ ê³µê³ ëŠ” h2 íƒœê·¸ ì‚¬ìš©
                        title_element = company_position_cell.find('h2', itemprop='title')
                        title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"
                    
                    # íšŒì‚¬ëª…(company) ì¶”ì¶œ
                    if is_ad:
                        # ê´‘ê³ ì˜ ê²½ìš° ë‘ ë²ˆì§¸ a íƒœê·¸ë‚˜ span íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                        company_elements = company_position_cell.find_all('a')
                        if len(company_elements) > 1:
                            company = company_elements[1].text.strip()
                        else:
                            span_element = company_position_cell.find('span')
                            company = span_element.text.strip() if span_element else "ê´‘ê³ ì£¼"
                    else:
                        # ì¼ë°˜ ê³µê³ ëŠ” h3 íƒœê·¸ ì‚¬ìš©
                        company_element = company_position_cell.find('h3', itemprop='name')
                        company = company_element.text.strip() if company_element else "íšŒì‚¬ëª… ì—†ìŒ"
                    
                    # ìœ„ì¹˜ì •ë³´(location)ì™€ ê¸‰ì—¬ì •ë³´(salary) ì¶”ì¶œ
                    location = "ì •ë³´ ì—†ìŒ"
                    salary = ""
                    
                    if not is_ad:
                        location_elements = company_position_cell.find_all('div', class_='location')
                        
                        # ìœ„ì¹˜ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
                        if location_elements:
                            # ì²« ë²ˆì§¸ ìœ„ì¹˜ ìš”ì†Œì—ì„œ êµ­ê°€ ì •ë³´ ì¶”ì¶œ ì‹œë„
                            location_text = location_elements[0].text.strip()
                            if location_text:
                                location = location_text
                            
                            # ê¸‰ì—¬ ì •ë³´ í™•ì¸ ë° ì¶”ì¶œ
                            for loc_element in location_elements:
                                if 'ğŸ’°' in loc_element.text:
                                    salary = loc_element.text.strip()
                                    break
                    
                    # íƒœê·¸(tags) ì •ë³´ ì¶”ì¶œ
                    tags = []
                    if not is_ad:
                        tags_cell = job_row.find('td', class_='tags')
                        if tags_cell:
                            tag_elements = tags_cell.find_all('div', class_='tag')
                            for tag_element in tag_elements:
                                h3_tag = tag_element.find('h3')
                                if h3_tag:
                                    tags.append(h3_tag.text.strip())
                    
                    # ê²Œì‹œ ë‚ ì§œ ì¶”ì¶œ
                    posted_date = ""
                    if not is_ad:
                        # data-epochì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                        epoch_timestamp = job_row.get('data-epoch', '')
                        if epoch_timestamp:
                            try:
                                posted_date = time.strftime('%Y-%m-%d', time.localtime(int(epoch_timestamp)))
                            except:
                                # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ time íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                                time_element = job_row.find('time')
                                if time_element:
                                    posted_date = time_element.text.strip()
                    
                    # ì¢…í•© ì •ë³´ ìƒì„±
                    job_data = JobDataRemoteOK(
                        title=title, 
                        company_name=company, 
                        location=location, 
                        link=url, 
                        salary=salary, 
                        tags=tags,
                        posted_date=posted_date,
                        is_ad=is_ad
                    )
                    jobs_db.append(job_data.to_list())
                    
                except Exception as e:
                    logger.error(f"êµ¬ì§ í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
                    
            logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•´ {len(jobs_db)}ê°œ ì‘ì—… ì°¾ìŒ")
            return jobs_db
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return jobs_db
            
    def save_to_csv(self, keyword):
        """ìŠ¤í¬ë˜í•‘í•œ êµ¬ì§ ì •ë³´ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        self.add_keywords_from_input(keyword)
        for keyword in self.keywords:
            try:
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ì‘ì—… ìŠ¤í¬ë˜í•‘ ì¤‘...")
                keyword_jobs = self.scrape_keyword(keyword)
                
                if not keyword_jobs:
                    logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                    
                with open(f"remoteok_{keyword}_jobs.csv", mode="w", encoding="utf-8-sig") as file:
                    writer = csv.writer(file)
                    # ì²« ë²ˆì§¸ ì‘ì—…ì—ì„œ JobData ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ í—¤ë” ê°€ì ¸ì˜¤ê¸°
                    job_data = JobDataRemoteOK(
                        keyword_jobs[0][0], keyword_jobs[0][1], keyword_jobs[0][2], keyword_jobs[0][3]
                    )
                    writer.writerow(job_data.get_headers())
                    for job in keyword_jobs:
                        writer.writerow(job)
                        
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ {len(keyword_jobs)}ê°œ ì‘ì—…ì´ CSV íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                
            except Exception as e:
                logger.error(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í‚¤ì›Œë“œ '{keyword}'): {e}") 